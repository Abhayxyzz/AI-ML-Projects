{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dataset"
      ],
      "metadata": {
        "id": "k65MsFqsmD-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N8zBu8gUldxI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef69a3c8-f3ed-4274-e8f9-9f7f445c5777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Libraries"
      ],
      "metadata": {
        "id": "8jQgeinDnm7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "m3IqZ_N-nmNf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "LHSYuX1qog4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Image Preprocessing"
      ],
      "metadata": {
        "id": "AZ1hql0ezhe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_set = tf.keras.utils.image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/Fruit_vegetable_Recognition/train',\n",
        "    labels = 'inferred',\n",
        "    label_mode = 'categorical',\n",
        "    class_names = None,\n",
        "    color_mode = 'rgb',\n",
        "    batch_size = 32,\n",
        "    image_size = (64,64),\n",
        "    shuffle = True,\n",
        "    seed = None,\n",
        "    validation_split = None,\n",
        "    subset = None,\n",
        "    interpolation = 'bilinear',\n",
        "    follow_links = False,\n",
        "    crop_to_aspect_ratio = False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT61d9VnolY1",
        "outputId": "1d5ca91a-e6f4-4a3f-f587-cd7107e26d40"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3115 files belonging to 36 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Validation Image Preprocessing"
      ],
      "metadata": {
        "id": "T6pmooE2znzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "validation_set = tf.keras.utils.image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/Fruit_vegetable_Recognition/validation',\n",
        "    labels = 'inferred',\n",
        "    label_mode = 'categorical',\n",
        "    class_names = None,\n",
        "    color_mode = 'rgb',\n",
        "    batch_size = 32,\n",
        "    image_size = (64,64),\n",
        "    shuffle = True,\n",
        "    seed = None,\n",
        "    validation_split = None,\n",
        "    subset = None,\n",
        "    interpolation = 'bilinear',\n",
        "    follow_links = False,\n",
        "    crop_to_aspect_ratio = False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhp96wJkymUy",
        "outputId": "9936d64e-571c-4f81-e37f-fe6e00f29ea7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 351 files belonging to 36 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building Model\n"
      ],
      "metadata": {
        "id": "Flnj2Gm3PnVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ],
      "metadata": {
        "id": "iGZyEs2nPtVJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building Convolutional Layer"
      ],
      "metadata": {
        "id": "3n_LS-J4RX44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,activation='relu', input_shape=[64,64,3]))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
      ],
      "metadata": {
        "id": "HqFph-VNRiRH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
      ],
      "metadata": {
        "id": "HblYJ4ywTOEz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Dropout(0.5)) #To avoid overfitting"
      ],
      "metadata": {
        "id": "dxV_KcVtTry_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "metadata": {
        "id": "YPp-iMxRUaEM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))"
      ],
      "metadata": {
        "id": "l2YRaeY6Umyx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Output Layer\n",
        "cnn.add(tf.keras.layers.Dense(units=36,activation='softmax')) #We've 36 classes"
      ],
      "metadata": {
        "id": "fUrA_c_TXEgS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compiling and Training Phase"
      ],
      "metadata": {
        "id": "z34AB475flTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "PvKNOlnafn7e"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_history= cnn.fit(x=training_set, validation_data=validation_set,epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VZU1JNRgma3",
        "outputId": "6800d870-7067-4f29-c435-282758ba91af"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "98/98 [==============================] - 474s 4s/step - loss: 9.2674 - accuracy: 0.0295 - val_loss: 3.5446 - val_accuracy: 0.0541\n",
            "Epoch 2/30\n",
            "98/98 [==============================] - 133s 1s/step - loss: 3.6649 - accuracy: 0.0324 - val_loss: 3.5283 - val_accuracy: 0.0541\n",
            "Epoch 3/30\n",
            "98/98 [==============================] - 127s 1s/step - loss: 3.7215 - accuracy: 0.0404 - val_loss: 3.5250 - val_accuracy: 0.0598\n",
            "Epoch 4/30\n",
            "98/98 [==============================] - 121s 1s/step - loss: 3.7208 - accuracy: 0.0523 - val_loss: 3.4455 - val_accuracy: 0.0826\n",
            "Epoch 5/30\n",
            "98/98 [==============================] - 129s 1s/step - loss: 3.6454 - accuracy: 0.0607 - val_loss: 3.4502 - val_accuracy: 0.0855\n",
            "Epoch 6/30\n",
            "98/98 [==============================] - 130s 1s/step - loss: 3.6778 - accuracy: 0.0729 - val_loss: 3.3978 - val_accuracy: 0.1254\n",
            "Epoch 7/30\n",
            "98/98 [==============================] - 137s 1s/step - loss: 3.6417 - accuracy: 0.0799 - val_loss: 3.3353 - val_accuracy: 0.1681\n",
            "Epoch 8/30\n",
            "98/98 [==============================] - 123s 1s/step - loss: 3.8352 - accuracy: 0.0831 - val_loss: 3.7514 - val_accuracy: 0.0598\n",
            "Epoch 9/30\n",
            "98/98 [==============================] - 132s 1s/step - loss: 3.6765 - accuracy: 0.0828 - val_loss: 3.1822 - val_accuracy: 0.1738\n",
            "Epoch 10/30\n",
            "98/98 [==============================] - 124s 1s/step - loss: 3.5244 - accuracy: 0.1056 - val_loss: 15.1780 - val_accuracy: 0.0370\n",
            "Epoch 11/30\n",
            "98/98 [==============================] - 131s 1s/step - loss: 3.6464 - accuracy: 0.0902 - val_loss: 3.1432 - val_accuracy: 0.1510\n",
            "Epoch 12/30\n",
            "98/98 [==============================] - 136s 1s/step - loss: 3.4743 - accuracy: 0.1037 - val_loss: 3.0688 - val_accuracy: 0.1937\n",
            "Epoch 13/30\n",
            "98/98 [==============================] - 135s 1s/step - loss: 3.2859 - accuracy: 0.1239 - val_loss: 3.3024 - val_accuracy: 0.1339\n",
            "Epoch 14/30\n",
            "98/98 [==============================] - 124s 1s/step - loss: 3.7535 - accuracy: 0.0719 - val_loss: 3.5863 - val_accuracy: 0.0285\n",
            "Epoch 15/30\n",
            "98/98 [==============================] - 124s 1s/step - loss: 3.6990 - accuracy: 0.0343 - val_loss: 3.5889 - val_accuracy: 0.0285\n",
            "Epoch 16/30\n",
            "98/98 [==============================] - 138s 1s/step - loss: 3.5838 - accuracy: 0.0324 - val_loss: 3.5864 - val_accuracy: 0.0285\n",
            "Epoch 17/30\n",
            "98/98 [==============================] - 124s 1s/step - loss: 3.5833 - accuracy: 0.0311 - val_loss: 3.5865 - val_accuracy: 0.0285\n",
            "Epoch 18/30\n",
            "98/98 [==============================] - 124s 1s/step - loss: 3.5805 - accuracy: 0.0311 - val_loss: 3.5865 - val_accuracy: 0.0285\n",
            "Epoch 19/30\n",
            "98/98 [==============================] - 133s 1s/step - loss: 3.5802 - accuracy: 0.0311 - val_loss: 3.5865 - val_accuracy: 0.0285\n",
            "Epoch 20/30\n",
            "98/98 [==============================] - 123s 1s/step - loss: 3.5799 - accuracy: 0.0311 - val_loss: 3.5867 - val_accuracy: 0.0285\n",
            "Epoch 21/30\n",
            "98/98 [==============================] - 130s 1s/step - loss: 3.5796 - accuracy: 0.0311 - val_loss: 3.5868 - val_accuracy: 0.0285\n",
            "Epoch 22/30\n",
            "98/98 [==============================] - 124s 1s/step - loss: 3.5800 - accuracy: 0.0311 - val_loss: 3.5869 - val_accuracy: 0.0285\n",
            "Epoch 23/30\n",
            "98/98 [==============================] - 133s 1s/step - loss: 3.5793 - accuracy: 0.0279 - val_loss: 3.5871 - val_accuracy: 0.0285\n",
            "Epoch 24/30\n",
            "98/98 [==============================] - 133s 1s/step - loss: 3.5791 - accuracy: 0.0276 - val_loss: 3.5872 - val_accuracy: 0.0285\n",
            "Epoch 25/30\n",
            "98/98 [==============================] - 130s 1s/step - loss: 3.5790 - accuracy: 0.0302 - val_loss: 3.5873 - val_accuracy: 0.0285\n",
            "Epoch 26/30\n",
            "98/98 [==============================] - 132s 1s/step - loss: 3.5789 - accuracy: 0.0315 - val_loss: 3.5874 - val_accuracy: 0.0285\n",
            "Epoch 27/30\n",
            "98/98 [==============================] - 131s 1s/step - loss: 3.5788 - accuracy: 0.0321 - val_loss: 3.5874 - val_accuracy: 0.0285\n",
            "Epoch 28/30\n",
            "98/98 [==============================] - 131s 1s/step - loss: 3.5824 - accuracy: 0.0321 - val_loss: 3.5876 - val_accuracy: 0.0285\n",
            "Epoch 29/30\n",
            "98/98 [==============================] - 132s 1s/step - loss: 3.5787 - accuracy: 0.0321 - val_loss: 3.5876 - val_accuracy: 0.0285\n",
            "Epoch 30/30\n",
            "98/98 [==============================] - 139s 1s/step - loss: 3.5787 - accuracy: 0.0321 - val_loss: 3.5877 - val_accuracy: 0.0285\n"
          ]
        }
      ]
    }
  ]
}